{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSE Data Science Institute | DS105W (2023/24) | Week 05\n",
    "\n",
    "# üóìÔ∏è Week 05: More Web scraping: CSS Selectors & XPaths\n",
    "\n",
    "Theme: Collecting Data\n",
    "\n",
    "**DATE:** 15 February 2024\n",
    "\n",
    "**AUTHOR:** Dr [Jon Cardoso-Silva](https://jonjoncardoso.github.io)\n",
    "\n",
    "-----\n",
    "\n",
    "**üìö LEARNING OBJECTIVES:**\n",
    "\n",
    "- Discover the basics of XPath and how it compares to CSS selectors\n",
    "- How to orient the web scraping process around **containers** instead of individual elements\n",
    "- How to write **custom functions** to extract data from a website\n",
    "\n",
    "**PRE-REQUISITES:**\n",
    "\n",
    "To understand this notebook better, you should first revisit the following notebooks: \n",
    "\n",
    "- üìö Week 04 Lecture Notebook\n",
    "- üõ£Ô∏è Week 05 Lab Roadmap\n",
    "- ‚úÖ Week 05 Lab Solutions\n",
    "\n",
    "**USEFUL LINKS:**\n",
    "\n",
    "- <span style=\"font-size:1.5em;\">Strong recommendation: üìò [HTML & CSS](https://wtf.tw/ref/duckett.pdf) book</span>\n",
    "- [W3 Schools - CSS Selectors](https://www.w3schools.com/CSS/css_selectors.asp)\n",
    "- [Complete list of CSS selectors](https://www.w3.org/TR/selectors-3/#selectors)\n",
    "- [`scrapy` extensions to CSS Selectors](https://docs.scrapy.org/en/latest/topics/selectors.html#extensions-to-css-selectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary style=\"display: list-item;cursor: pointer;\"><h2 style=\"display:inline;border-bottom: 1px solid #dee2e6;padding-bottom: .5rem;margin-left:1rem;font-weight: 300;font-size:1.5rem;font-family: 'News Cycle','Arial Narrow Bold',sans-serif;line-height: 1.1;vertical-align:middle;color:#c89020\">CSS selectors cheatsheet ‚≠ê</h2></summary> \n",
    "\n",
    "<div style=\"margin-top:1.5em;width:80%;font-size:0.9em;\">\n",
    "\n",
    "| Selector              | Example                  | Use Case Scenario                                                                                                                              |\n",
    "|-----------------------|--------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| *                     | *                        | This selector picks all elements within a page. It‚Äôs not that different from a page. Not much use for it but still good to know                |\n",
    "| .class                | .card-title              | The simplest CSS selector is targeting the class attribute. If only your target element is using it, then it might be sufficient.            |\n",
    "| .class1.class2        | .card-heading.card-title | There are elements with a class like class=‚Äúcard-heading card-title‚Äù. When we see a space, it is because the element is using several classes. However, there‚Äôs no one fixed way of selecting the element. Try keeping the space, if that doesn‚Äôt work, then replace the space with a dot. |\n",
    "| #id                   | #card-description        | What if the class is used in too many elements or if the element doesn‚Äôt have a class? Picking the ID can be the next best thing. The only problem is that IDs are unique per element. So won‚Äôt cut to scrape several elements at once.                   |\n",
    "| element               | h4                       | To pick an element, all we need to add to our parser is the HTML tag name.                                                                  |\n",
    "| element.class         | h4.card-title            | This is the most common we‚Äôll be using in our projects.                                                                                      |\n",
    "| parentElement > childElement | div > h4          | We can tell our scraper to extract an element inside another. In this example, we want it to find the h4 element whose parent element is a div.                                                             |\n",
    "| parentElement.class > childElement | div.card-body > h4 | We can combine the previous logic to specify a parent element and extract a specific CSS child element. This is super useful when the data we want doesn‚Äôt have any class or ID but is inside a parent element with a unique class/ID. |\n",
    "| [attribute]           | [href]                   | Another great way to target an element with no clear class to choose from. Your scraper will extract all elements containing the specific attribute. In this case, it will take all <a> tags which are the most common element to contain an href attribute. |\n",
    "| [attribute=value]     | [target=_blank]          | We can tell our scraper to extract only the elements with a specific value inside its attribute.                                              |\n",
    "| element[attribute=value] | a[rel=next]          | This is the selector we used to add a crawling feature to our Scrapy script: next_page = response.css(‚Äòa[rel=next]‚Äô).attrib[‚Äòhref‚Äô] The target website was using the same class for all its pagination links so we had to come up with a different solution. |\n",
    "| [attribute~=value]    | [title~=rating]         | This selector will pick all the elements containing the word ‚Äòrating‚Äô inside its title attribute.                                             |\n",
    "\n",
    "</div>\n",
    "\n",
    "Source: [The Only CSS Selectors Cheat Sheet You Need for Web Scraping](https://www.scraperapi.com/blog/css-selectors-cheat-sheet/#CSS-Selectors-Cheat-Sheet)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from scrapy import Selector\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above throws an error of module not found, open the terminal and type `pip install tqdm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OBJECTIVE:**\n",
    "\n",
    "In this tutorial, we will revisit the same exercise from the W05 lab, only this time we will solve it with the help of XPath instead of CSS selectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The (X)Path to success\n",
    "\n",
    "We have seen how we can use CSS selectors to find elements in the HTML code of a webpage. However, there is another way to do this: **XPath**. \n",
    "\n",
    "<div style=\"width:70%;border: 1px solid #aaa; border-radius:1em; padding: 1em; margin: 1em 0;\">\n",
    "\n",
    "üìï **What is XPath?** \n",
    "\n",
    "XPath is a query language for selecting nodes from an XML language\n",
    "\n",
    " XPath is a bit more powerful than CSS selectors, but it is also a bit more complicated. It allows us more flexibility when it comes to finding the children or partents of certain elements. What's more, it you find yourself being able to get to a parent element with CSS but cannot get to its children because they have some obscure class name, XPath can be 'chained' at the end of a CSS element. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Loading the page and getting the HTML\n",
    "\n",
    "First, we send a GET request to the page to obtain a response object, with which we can get the HTML code of the page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://socialdatascience.network/index.html#schedule'\n",
    "\n",
    "# Load the first page\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:70%;border: 1px solid #aaa; border-radius:1em; padding: 1em; margin: 1em 0;\">\n",
    "\n",
    "üí° **PRO-TIP:** \n",
    "\n",
    "In reality, `requests.get(url)` is a shortening of `requests.request('GET', url)`.\n",
    "\n",
    "Whenever we send a request to a server, we need to specify the type of request we are sending. The most common types are GET, which intuitively means that we are asking the server to send us some data, and POST, which means that we are sending some data to the server.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right after collecting the page, it's a good practice to check if the request was successful. We can do this by checking the status code of the response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the status code, 200 means OK, anything else means something went wrong\n",
    "if not response.status_code == 200:\n",
    "    print('Something went wrong, status code:', response.status_code)\n",
    "else:\n",
    "    print('Everything is OK, status code:', response.status_code)\n",
    "\n",
    "# print the HTML code\n",
    "print(response.text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width:80%;font-size:0.85em;border: 1px solid #aaa; border-radius:1em; padding: 1em; margin: 1em 0;\">\n",
    "\n",
    "**Did you get a 403 error?**\n",
    "\n",
    "If, instead, you got an 403 error at this stage, it's probably because the server is refusing to respond to our request because it thinks we are a 'robot', not a user accessing from a web browser. They are not entirely wrong...\n",
    "\n",
    "When we send a request, the server searches for a header (a metadata) in our request called `User-Agent`. Browsers have unique `User-Agent` headers that uniquely identifies the browser and the operating system that is being used. For example:\n",
    "\n",
    "- Chrome on Windows 10: `Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36`\n",
    "\n",
    "- Firefox on Windows 10: `Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:93.0) Gecko/20100101 Firefox/93.0`\n",
    "\n",
    "- Chrome on Mac OS: `Mozilla/5.0 (Macintosh; Intel Mac OS X 11_6_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36`\n",
    "\n",
    "A server might be configured to block the request when it doesn't come from a standard browser. We can fix this by amending our request, specifying a `User-Agent` header, and then passing it to the `requests.get()` function. So far though, it looks like we are getting the HTML code of the page, so we can move on to the next step.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Creating a Selector object\n",
    "\n",
    "Now that we have the HTML code, we can create a scrapy `Selector` object from it. This will allow us to use CSS and XPath selectors to find elements in the HTML code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Selector object from the HTML code\n",
    "sel = Selector(text = response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You know we can now use the `sel.css()` method to find elements in the HTML code using CSS selectors. \n",
    "\n",
    "Let's practice with the `*` selector, which selects all elements in the HTML code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print number of HTML elements returned by the CSS selector\n",
    "print('Number of elements on the page with CSS:', len(sel.css('*')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same with XPath selectors, using the `sel.xpath()` method. The syntax is a bit different, but the result is the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the number of elements on the page\n",
    "print('Number of elements on the page with XPath:', len(sel.xpath('//*')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. CSS vs XPath selectors\n",
    "\n",
    "## 2.1 Inspecting the elements\n",
    "\n",
    "Let's start by inspecting the elements on the page. As we saw last week, we can inspect the elements on the page by right-clicking on the page and selecting 'Inspect'. (You can achieve the same by pressing `Ctrl+Shift+I` on Windows or `Cmd+Shift+I` on Mac)\n",
    "\n",
    "Once you have the developer tools open, hover over the elements on the page to see their HTML code:\n",
    "\n",
    "![](./figures/civica_screenshot.png)\n",
    "\n",
    "Let's compare how we can get this element with CSS and XPath selectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. CSS selectors\n",
    "\n",
    "We learned that the titles of events are stored inside `h6` tags nested inside the link (`a`) tags, nested inside a `div` tag with the class `card-body`.\n",
    "\n",
    "Using CSS selectors, we specify the tag name, the class with the `.` operator and the child element with the `>` operator:\n",
    "\n",
    "```css\n",
    "div.card-body > a > h6\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the HTML code of the element with CSS selectors\n",
    "pprint(sel.css('div.card-body > a > h6').extract_first()) # this gets the first element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract ALL the elements with this class, use the `extract()` method after the `sel.css()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The getall() method returns a list of strings\n",
    "titles = sel.css('div.card-body > a > h6').extract()\n",
    "print('There are', len(titles), 'event titles on the page.') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OK, but this returns the tag itself. I only care about the text inside the tag.**\n",
    "\n",
    "For this, we use [`scrapy`'s extension](https://docs.scrapy.org/en/latest/topics/selectors.html#extensions-to-css-selectors) `::text`:\n",
    "\n",
    "```css\n",
    "div.card-body > a > h6::text\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 XPath\n",
    "\n",
    "The syntax resembles a bit the way we would write a path to a file in the **Terminal**. You can use the familiar `..` and `/` symbols when navigating through the HTML code.\n",
    "\n",
    "### 2.3.1 Absolute paths\n",
    "\n",
    "\n",
    "For example, a single forward slash `/` at the beginning of the path indicates that you're looking for elements within the current element. When we get a `sel` object, we are looking at the entire HTML code of the page, so the following syntax:\n",
    "\n",
    "```xpath\n",
    "/html/head/title\n",
    "```\n",
    "\n",
    "will return the tag that contains the title of the page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel.xpath('/html/head/title').extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {style=\"margin-left:1.5em; background-color: #f9f9f9; border: 1px solid #ddd; border-radius: 0.5em; padding: 1em;margin-bottom:1.5em;\"}\n",
    "\n",
    "**How do I test that on the browser, without having to write Python code?**\n",
    "\n",
    "On the Inspector, you can click on the 'Console' tab and type the following:\n",
    "\n",
    "```javascript\n",
    "$x('/html/head/title')\n",
    "```\n",
    "\n",
    "This will return an Array (similar to a Python list) that contains the element we are looking for. Take a look at the video below for a demonstration:\n",
    "\n",
    "<div style=\"position: relative; padding-bottom: 59.375%; height: 0;\"><iframe src=\"https://www.loom.com/embed/5c324141c4c24a6fb1ce7417c84ec422?sid=3d1cd528-fa33-4c55-ae25-adc0aeb6f0a6\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%;\"></iframe></div>\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Relative paths\n",
    "\n",
    "Just like with CSS selector, you don't need to specify the entire path to the element you are looking for. You can use the `//` symbol to indicate that you are looking for an element anywhere in the HTML code. For example, the following XPath selector:\n",
    "\n",
    "```xpath\n",
    "//title\n",
    "```\n",
    "would also return the `<title>` tag.\n",
    "\n",
    "Give it a go! Open the browser console and type:\n",
    "\n",
    "```javascript\n",
    "$x('//title')\n",
    "```\n",
    "\n",
    "To confirm that you got the same result as before.\n",
    "\n",
    "Perhaps more usefully, use `.outerHTML` to see the entire HTML code of the element:\n",
    "\n",
    "```javascript\n",
    "$x('//title')[0].outerHTML // ONLY WORKS IN THE BROWSER\n",
    "```\n",
    "\n",
    "Like with CSS selectors, the search will not stop at the first element that matches the selector. It will return all the elements that match the selector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Selecting elements by class\n",
    "\n",
    "Instead of the `.` symbol used in CSS selectors, we use a mix of `contains()` and `@class` to select elements by class:\n",
    "\n",
    "This is the syntax to select `<div>` elements with a `card__content` class:\n",
    "\n",
    "```css\n",
    "//div[contains(@class, 'card-body')]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the HTML code of all elements with XPath selectors\n",
    "xpath_event_divs = '//div[contains(@class, \"card-body\")]'\n",
    "xpath_all_events = sel.xpath(xpath_event_divs).extract()\n",
    "print('There are', len(xpath_all_events), 'events on the page.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to see the first event with all details\n",
    "#print(xpath_all_events[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4 Of parents and children: the power of XPath\n",
    "\n",
    "As mentioned above, XPath behaves a bit like a file system. We can use the `/..` symbol to get the parent of an element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's another way to grab that `div.card-body` element\n",
    "print(sel.xpath('//h6/../..').extract_first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **PRO-TIP:** You can index the results with XPath. Say you only care about the first match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it: replace [1] with [2] or any other number to get the corresponding element\n",
    "indexed_div = sel.xpath('(//div[contains(@class, \"card-body\")])[1]').extract()\n",
    "\n",
    "# I still used extract() instead of extract_first() \n",
    "# to show you that it returns a list with a single element\n",
    "indexed_div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we get **just the children** of an element? We use the `/*` symbol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "children_of_div = sel.xpath('(//div[contains(@class, \"card-body\")])[1]/*').extract()\n",
    "\n",
    "children_of_div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You could not have done this with CSS selectors!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do I get the text inside the tag? We use the `text()` method at the end (similarly to `::text` in CSS selectors):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel.xpath('(//div[contains(@class, \"card-body\")])[1]//h6/text()').extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do I get attributes? We use the `@` symbol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel.xpath('(//a)[10]/@href').extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What else can I do?**\n",
    "\n",
    "- [W3 Schools XPath Tutorial](https://www.w3schools.com/xml/xpath_intro.asp)\n",
    "- Check out this [old school XPath Tutorial](http://www.zvon.org/comp/r/tut-XPath_1.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 More power to you: chaining CSS and XPath selectors\n",
    "\n",
    "Note the different syntax for CSS and XPath selectors. CSS selectors are more straightforward, but XPath selectors are more powerful.\n",
    "\n",
    "üí° **PRO-TIP:** What is really nice about `scrapy` is that you can chain CSS and XPath selectors together! This means you can use a CSS selector to get an element and an XPath selector to get its children.\n",
    "\n",
    "Remember from the ‚úÖ [Week 05 lab solutions](https://lse-dsi.github.io/DS105/2023/winter-term/weeks/week05/lab-solutions.html) that we used XPath to get speakers and dates:\n",
    "\n",
    "```python\n",
    "speakers_xpath = \"//p[@class='card-text']/text()[1]\"\n",
    "speakers = selector.xpath(speakers_xpath).extract()\n",
    "\n",
    "dates_xpath = \"//p[@class='card-text']/text()[2]\"\n",
    "dates = selector.xpath(dates_xpath).extract()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could I do this with a mix CSS and XPath selectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers = sel.css('div.card-body > p').xpath('text()[1]').extract()\n",
    "dates    = sel.css('div.card-body > p').xpath('text()[2]').extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, it is up to you to decide which one you prefer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Orienting the scraping process around containers\n",
    "\n",
    "\n",
    "We have already talked about `pandas` but haven't done much with them at this stage in the course. This is because we're focused on **collecting** data. Once we have enough data and are ready to work with it, you will see how tables (data frames) provide an excellent and very convenient way to store data for analysis.\n",
    "\n",
    "**In the meantime, trust me! You want to convert whatever data you capture into a pandas data frame.**\n",
    "\n",
    "With what we have learned so far, it is easy to convert the event details we care about into a pandas data frame. We can summarise the whole process as:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture all the info individually\n",
    "titles = sel.css('div.card-body > a > h6').extract()\n",
    "speakers = sel.css('div.card-body > p').xpath('text()[1]').extract()\n",
    "dates = sel.css('div.card-body > p').xpath('text()[2]').extract()\n",
    "\n",
    "# Put it all together into a DataFrame\n",
    "events = pd.DataFrame({'title': titles, 'speaker': speakers, 'date': dates})\n",
    "\n",
    "# Uncomment to browse the DataFrame\n",
    "#events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the above works, it can lead to much repetition and 'workaround' code if the website you are scraping is not the most consistent. For example, some event divs may have a different class name, or some events list the speakers and dates in the opposite order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Treat each event box as a template\n",
    "\n",
    "Instead of going directly to the titles, speakers, and dates, we can capture the `<div>` that represents an event and then extract the details from its children."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture the divs but don't extract yet!\n",
    "containers = sel.css('div.card-body')\n",
    "\n",
    "len(containers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "containers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note one very important thing about the `containers[0]` object: it is a `Selector` object, not a string. Therefore, it contains other methods and attributes inside it.\n",
    "\n",
    "**Why is that useful?** \n",
    "\n",
    "We can treat this object as our HTML code, ignore the rest of the HTML, and use the same methods to get the information we need from it. We don't need to scrape the entire page again to get the necessary information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Extract all details of a single event\n",
    "\n",
    "To illustrate the observation above, let's get the title, speaker and date of _this particular event_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_title   = containers[0].css('h6::text').extract_first()\n",
    "event_speaker = containers[0].xpath('p/text()[1]').extract_first()\n",
    "event_date    = containers[0].xpath('p/text()[2]').extract_first() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Putting it all together (NOT ELEGANT)\n",
    "\n",
    "\n",
    "I know what you are thinking... _'but then I will have to do this for every event!'_\n",
    "\n",
    "True. If you were to proceed with the above and use what you learned from your previous Python training, you would have to write a loop that goes through each event and extracts the details.\n",
    "\n",
    "It's likely that you would be tempted to write a code that looks like this:\n",
    "\n",
    "```python\n",
    "#### WE DON'T WANT YOU TO WRITE THIS TYPE OF CODE IN THIS COURSE! I'LL EXPLAIN IN SECTION 3.3 ####\n",
    "\n",
    "\n",
    "# Create an empty list to store the details of each event\n",
    "event_titles = []\n",
    "event_speakers = []\n",
    "event_dates = []\n",
    "\n",
    "# Loop through each event\n",
    "for container in containers:\n",
    "    # Extract the details of the event\n",
    "    title = container.css('h6::text').extract_first()\n",
    "    speaker = container.xpath('p/text()[1]').extract_first()\n",
    "    date = container.xpath('p/text()[2]').extract_first()\n",
    "\n",
    "    # Append the details to the lists\n",
    "    event_titles.append(title)\n",
    "    event_speakers.append(speaker)\n",
    "    event_dates.append(date)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Using custom functions (ELEGANT üé©)\n",
    "\n",
    "What we want is for you to use best practices when writing code. Code that is efficient, easy to read, and easy to alter in the future. With practice, you will realise that `for` loops are not always the best way to go about things. If you find a bug in the code above, you would have to go through the entire loop to locate the source of the bug.\n",
    "\n",
    "**Functions** (with the `def` operator) are a great way to encapsulate a piece of code that does a single task. You can run the same function with different parameters to test out different scenarios. If you find a bug in the function, you only have to fix it once.\n",
    "\n",
    "How would a function look like for the above?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_event(event_container):\n",
    "    event_title   = event_container.css('h6::text').extract_first()\n",
    "    event_speaker = event_container.xpath('p/text()[1]').extract_first()\n",
    "    event_date    = event_container.xpath('p/text()[2]').extract_first()\n",
    "    return {'title': event_title, 'speaker': event_speaker, 'date': event_date}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way, you can test the function for individual containers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change [0] to [1] or any other number to get the corresponding event\n",
    "scrape_event(containers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice that we returned a dictionary instead of a list.** Key-value pairs are the most natural way to store a single record of data. \n",
    "\n",
    "If we add up all the dictionaries, we get a list of dictionaries, making it easier to convert to a pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape all events (note that we're using list comprehension)\n",
    "events = [scrape_event(container) for container in containers]\n",
    "\n",
    "# Creating a dataframe is easier\n",
    "df = pd.DataFrame(events)\n",
    "\n",
    "# Uncomment to browse the DataFrame\n",
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Writing Great Documentation\n",
    "\n",
    "**Your future self (and the reviewers of your code) will thank you for it!**\n",
    "\n",
    "Writing maintainable code is not just about writing code that works. It's about writing code that is easy to understand and easy to alter in the future.\n",
    "\n",
    "\n",
    "<div style=\"width:70%;border: 1px solid #aaa; border-radius:1em; padding: 1em; margin: 1em 0;\">\n",
    "\n",
    "üí° **TIP:** Always think to yourself: what comments or documentation can I add to my code so that if I return to it in a few months, I would still understand what I was trying to do?\n",
    "\n",
    "</div>\n",
    "\n",
    "One excellent way to document your code is to write a [**docstring**](https://realpython.com/documenting-python-code/). A docstring is a string that comes right after the `def` operator and describes what the function does. Writing a docstring for every function you write is an excellent practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_event(event_container):\n",
    "    \"\"\"Scrape details from a single event container.\n",
    "    \n",
    "    An event container looks like this:\n",
    "    <div class=\"card-body\">\n",
    "        <a href=\"...\">\n",
    "            <h6>Event title</h6>\n",
    "        </a>\n",
    "        <p>Speaker name</p>\n",
    "        <p>Date</p>\n",
    "    </div>\n",
    "\n",
    "    This function captures the title, speaker, and date from the container.\n",
    "    \n",
    "    Args:\n",
    "        event_container (Selector): a Selector object with the HTML of the event container\n",
    "\n",
    "    Returns:\n",
    "        dict: a dictionary with the title, speaker, and date of the event\n",
    "    \"\"\"\n",
    "\n",
    "    event_title   = event_container.css('h6::text').extract_first()\n",
    "    event_speaker = event_container.xpath('p/text()[1]').extract_first()\n",
    "    event_date    = event_container.xpath('p/text()[2]').extract_first()\n",
    "    return {'title': event_title, 'speaker': event_speaker, 'date': event_date}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I could also encapsulate the whole process of collecting the containers into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_events(url):\n",
    "    \"\"\"Scrape all events from a given URL.\n",
    "    \n",
    "    Args:\n",
    "        url (str): the URL of the page with events\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: a DataFrame with the title, speaker, and date of each event\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the page\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if not response.status_code == 200:\n",
    "        print('Something went wrong, status code:', response.status_code)\n",
    "        return\n",
    "\n",
    "    sel = Selector(text = response.text)\n",
    "\n",
    "    # Capture the divs but don't extract yet!\n",
    "    containers = sel.css('div.card-body')\n",
    "\n",
    "    # Scrape all events\n",
    "    events = [scrape_event(container) for container in containers]\n",
    "\n",
    "    # Creating a dataframe is easier\n",
    "    df = pd.DataFrame(events)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In theory, you wouldn't even keep the code above inside a notebook like this. You would write it in a `.py` file and import the function when needed.\n",
    "\n",
    "Your final code would then look like this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://socialdatascience.network/index.html#schedule'\n",
    "df = scrape_events(url)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neat!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What to do next\n",
    "\n",
    "- Go through all the reference links in this notebook to recap CSS and XPath selectors\n",
    "- Try incorporating the notions above into your üìù **W06 Summative (30%)** submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
